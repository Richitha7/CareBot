{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTeoZRKFFkPr"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MObgcFCm6S_f"
      },
      "outputs": [],
      "source": [
        "!pip install datasets tqdm pandas matplotlib langchain sentence_transformers faiss-gpu langchain-community torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KFrD7tdCmZC"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I358tJXM6rhS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "# Set display option for pandas\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# Open and read the first file\n",
        "with open(\"/content/only_answers_formatted.txt\", \"r\") as fp1:\n",
        "    s = fp1.read()\n",
        "\n",
        "\n",
        "# Split the combined content into sections\n",
        "#s = combined_content.split(\"\\n\\n\\n\")\n",
        "\n",
        "# Print the first section and the number of sections\n",
        "print(s[0])\n",
        "print(len(s))\n",
        "\n",
        "# Create a RAW_KNOWLEDGE_BASE using LangchainDocument\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc)\n",
        "    for doc in tqdm(s)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVNUlCSD6vNG"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6}\",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n__+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=MARKDOWN_SEPARATORS,\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "\n",
        "fig = pd.Series(lengths).hist()\n",
        "fig.set_title(\"Histogram of Document Lengths\")\n",
        "plt.title(\"Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE3v0G5G6z7f"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "def split_documents(\n",
        "        chunk_size: int,\n",
        "        knowledge_base: list[LangchainDocument],\n",
        "        tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "    return docs_processed_unique\n",
        "\n",
        "docs_processed = split_documents(512, RAW_KNOWLEDGE_BASE, tokenizer_name=EMBEDDING_MODEL_NAME)\n",
        "print(len(docs_processed))\n",
        "print(docs_processed[0:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H71jikz623d"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed,\n",
        "    embedding_model,\n",
        "    distance_strategy=DistanceStrategy.COSINE,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgRI9m_77fj3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO-r6mhF7f_V"
      },
      "outputs": [],
      "source": [
        "prompt_chat=[\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "        \"content\":\"\"\"Using the information contained in the context,\n",
        "Give a comprehensive answer to the question.\n",
        "Respond only to the question asked , response should be concise and relevant to the question.\n",
        "provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer\"\"\",\n",
        "\n",
        "    },\n",
        "    {\n",
        "        \"role\":\"user\",\n",
        "        \"content\":\"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the Question you need to answer.\n",
        "Question:{question}\n",
        "        \"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_chat,tokenize = False,add_generation_prompt=True,\n",
        "\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01pT1aVJswbT"
      },
      "outputs": [],
      "source": [
        "u_query = \"i have muscle pain\"\n",
        "# ret_text = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=u_query,k=3)\n",
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=u_query,k=3)\n",
        "\n",
        "context = retrieved_docs[0].page_content\n",
        "final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
        "    question= u_query, context = context\n",
        ")\n",
        "\n",
        "output = pipe(final_prompt, **generation_args)\n",
        "print(\"YOUR QUESTION:\\n\",u_query,\"\\n\")\n",
        "print(\"MICROSOFT 128K ANSWER: \\n\",output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmPGjMplpkJb"
      },
      "outputs": [],
      "source": [
        "pip install fastapi uvicorn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mj9RQhlpkaC"
      },
      "outputs": [],
      "source": [
        "#!ngrok config add-authtoken 2j6RAXfPqnZGd5s0A81p8K9kTTr_7qgYLG37aNBMMBS9yMR6P"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2j6aBw4LlFdyIJUWYG99x5lxUWt_6WceUU6tcbt7E7eSKT1hT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBv2aneWpuJw",
        "outputId": "5ff8be57-9e73-40d5-91b8-4a0aadd8fb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HADw1M6epkno"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Set up CORS middleware to allow all origins for development purposes\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allows all origins\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],  # Allows all methods\n",
        "    allow_headers=[\"*\"],  # Allows all headers\n",
        ")\n",
        "\n",
        "class QueryModel(BaseModel):\n",
        "    query: str\n",
        "\n",
        "\n",
        "@app.post(\"/query\")\n",
        "async def query_api(request: QueryModel):\n",
        "    u_query = request.query\n",
        "    retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=u_query, k=3)\n",
        "    context = retrieved_docs[0].page_content\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(\n",
        "        question=u_query, context=context\n",
        "    )\n",
        "    output = pipe(final_prompt, **generation_args)\n",
        "    return {\"question\": u_query, \"answer\": output[0]['generated_text']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHEzRM6qpkzm",
        "outputId": "0cfb4e4b-efc1-45bf-a0b5-5effad4aeebc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [244]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://8244-34-125-95-91.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "INFO:     152.58.196.151:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"OPTIONS /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"OPTIONS /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"OPTIONS /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"OPTIONS /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"OPTIONS /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:1148:66a7:7dbc:d78f:1032:8356:0 - \"POST /query HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# Necessary to run uvicorn in Jupyter notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Start FastAPI app\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}